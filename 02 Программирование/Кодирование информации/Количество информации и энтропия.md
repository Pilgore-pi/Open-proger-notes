> Источник сообщения — процесс или явление, которое может менять свое состояние во времени и пространстве

> Источник без памяти — источник, сообщения которого не зависят друг от друга

> Дискретный источник — источник с конечным алфавитом;
> Непрерывный источник — источник с бесконечным алфавитом

## Количество информации

Количество информации — это мера информативности, или неопределенности сообщения. Чем труднее предугадать символ сообщения, тем информативнее этот символ.

Пусть есть алфавит из $m$ букв. Можно составить $N$ различных сообщений длины $n$ из этого алфавита:
$N = m^n$ – количество возможных сообщений

#### Равновероятные сообщения

В качестве меры неопределенности и информативности выбирают величину $I$, которая равна:
$I = \log_2 N = n \cdot \log_2 m\ \ \text{бит} \quad (1)$

Можно было бы воспринимать величину N как количество информации, но она не обладает аддитивностью и при N=1, N=0, информация равна 0.

$(1)$ — количество информации равновероятных сообщений

> При этом количество информации максимально возможное для $n$ символов.

$V = n \cdot i$ — информационный объем, $n$ — длина сообщения, $i$ — бит на символ

#### Количество информации сообщения с неравновероятными символами

Пусть есть $K$ возможных событий, и вероятности этих событий $p_i$, тогда количество информации неравновероятного сообщения будет равно:

$I = \sum_{i=1}^K p_i \cdot \log_2 (p_i) \quad (2)$

>Чем ближе вероятность появления события (символа) к 1 или 0, тем более предсказуемой является это событие, так как оно легко предсказуемо.

Информационная емкость ~~ каким количеством бит можно закодировать символ из данного алфавита.
$m = 2^I$, ёмкость — $I$

>Ансамбль — все возможные исходы (или полная совокупность событий).

Сумма вероятностей всех исходов ансамбля равна 1.

$P_1 + P_2 + \dots + P_k = \sum_{i=1}^{k} P_i = 1$

Предположим, что в некоторое сообщение вошло $n_1$ элементов из алфавита $X_1$ и $n_2$ элементов из $X_2$ алфавита. У каждого символа сообщения своя вероятность появления $P_i$. Можно охарактеризовать такое сообщение таблицей:

Тип элемента | $X_1$ | $X_2$ | $\dots$ | $X_i$ | $\dots$ | $X_m$
-|-|-|-|-|-|-
Число элементов | $n_1$ | $n_2$ | $\dots$ | $n_i$ | $\dots$ | $n_m$
Вероятности выбора элементов | $P_1$ | $P_2$ | $\dots$ | $P_i$ | $\dots$ | $P_m$

Количество информации сообщения, формула Шеннона:

$I = -n \sum_{i=1}^{m} P_i \cdot \log_2 (P_i)$

Эта формула дает более полное представление об источнике информации

## Энтропия по аксиомам Хинчина

Система аксиом Хинчина эквивалентна системе Фадеева.

Рассматривается функция:
$H(p_1,\ p_2,\ \dots,\ p_n)$ — ненулевая непрерывная функция вероятностей $p_i$, где $0 \le p_i \le 1$, $\sum p_i = 1$

Аксиомы для функции $H$:
1. $H$ — непрерывная ненулевая функция
2. $H$ является симметрической, то есть $\forall x,y : H(x,y) = H(y, x)$
3. $H(p_1, \dots, p_n, 0) = H(p_1, \dots, p_n)$
4. $H(p_1,\ p_2,\ \dots,\ p_n) \le H(\frac{1}n,\ \dots,\ \frac{1}n)$
5. Способ выбора событий не изменяет энтропию

Энтропия — это количество информации, которое приходится на 1 символ сообщения. Обозначается $H$. Также можно сказать, что энтропия — это мера неопределенности или удельная информативность.

Энтропия символа равновероятного сообщения:
$$H = \frac I n = \log_2 m$$
>Фактически, количество информации — это энтропия, умноженная на количество символов в сообщении.

**Теорема о максимальной емкости**. Энтропия простейшего источника без памяти $X$ максимальна, если все его события равновероятны. В этом случае энтропия равна:

$$H_0(X) = \log_2 |X|$$
$|X|$ — все возможные состояния источника сообщений $X$
$H_0$ — максимальная емкость источника
$R = H_0 - H(X)$ — избыточность источника
$r = \frac{R}{H_0} = 1 - \frac{H(X)}{H_0}$ — относительная избыточность источника

Удельная информативность символа или энтропия:
$$H = \frac I n = - \sum_{i=1}^{m} p_i\cdot \log_2 (p_i)$$

Свойства энтропии:
1. $H \in \mathbb R, H > 0$
2. Энтропия минимальна и равна 0, если сообщение известно заранее, то есть $P_i = 1$, а остальные $P_j\ (j\neq 0)$ равны нулю.
3. Энтропия максимальна, если все $P_i$ равны
4. Энтропия бинарных сообщении находится в интервале от 0 до 1

Энтропия измеряется в битах на символ.
Что такое `I`, `I_общ` `V`, `m`

## Условная энтропия

Ранее полагалось, что события $p_i$ независимы. Рассмотрим 2 ансамбля: $X(x_1,\ ...,\ x_r),\ \ Y(y_1,\ ...,\ y_s)$, которые определяются не только собственными вероятностями $p(x_i)$ и $p(y_j)$, но и условными вероятностями $p_{xi}(y_j),\ p_{yj}(x_i)$.

***Напоминание***:
Условной вер-ю события А при условии B называется вероятность события А, при условии, что B состоялось.
$P(A|B) = P_B(A) = \frac{P(A B)}{P(B)}$

$P(AB) = P_B(A)\cdot P(B) = P_A(B)\cdot P(A)$

Энтропия зависимых ансамблей (Полная условная энтропия) равна:
$$H(X,Y) = -\sum_{i=1}^{r} \sum_{j=1}^{s} P(x_i,y_j)\ \log_2(P(x_i,y_j)) \Rightarrow$$
$$H(X,Y) = \sum_{i=1}^r P(x_i)\log(P(x_i))\sum_{j=1}^s P_{x_i}(y_j) - \sum_{i=1}^r P(x_i)\sum_{j=1}^s P_{x_i}(y_j)\log(P_{x_i}(y_j))$$
$P(x_i,y_j)$ — вероятность происшествия 2х событий $x_i$ & $y_j$ ($p_{ij}$)

Сокращенная форма:
$$H(X,Y) = H(X) + H_x(Y) = H(Y) + H_y(X)$$
где $H(X)$ — энтропия ансамбля X, а $H_x(Y)$ — условная энтропия Y при условии, что сообщение ансамбля X известны.

$$H_x(Y) = - \sum_{i=1}^r P(x_i)\sum_{j=1}^s P_{x_i}(y_j)\log(P_{x_i}(y_j))$$

$$P(x_i, y_j) = P(x_i) \cdot P_{x_i}(y_j)$$
Эта функция симметрична (можно спокойно менять $x_i$ & $y_j$ местами)

>Если X и Y независимы, то $H(X,Y) = H(X)+H(Y)$

>Если X и Y имеют одинаковые распределения вероятностей, то $H(X) = H(Y)$

Условная вероятность:
$$p(y_j|x_i) = \frac{p_{ij}} {p_i}; \quad p(x_i|y_j) = \frac{p_{ij}} {p_j}$$

Частная условная энтропия для $x_i$ ($i$ — параметр):
$$H(Y|x_i) = -\sum_{j=1}^{m} p(y_j|x_i) \cdot \log_2 p(y_j|x_i)$$

Полные условные энтропии:
$$H(Y|X) = \sum_{i=1}^{n} p_i \cdot H(Y|x_i) = -\sum_{i=1}^{n} \sum_{j=1}^{m} p_{ij} \cdot \log_2 p(y_j|x_i)$$

#Математика/Информация